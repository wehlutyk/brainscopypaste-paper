\documentclass[a4paper,10pt]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{utopia}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[authoryear,round,compress]{natbib}
\usepackage{multicol}
\usepackage[usenames,dvipsnames]{color}
\usepackage{url}
\usepackage{eurosym}
\usepackage{array}
\usepackage{tabularx}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{wrapfig}

\usepackage{float}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[frenchb]{babel}
\usepackage{setspace}
\setstretch{1.05}

\newcommand{\itx}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\enx}[1]{\begin{enumerate}#1\end{enumerate}}
\newcommand{\x}{\item}


\usepackage[bookmarks, colorlinks, breaklinks, pdftitle={Research project},pdfauthor={Camille Roth}]{hyperref}  
\hypersetup{linkcolor= MidnightBlue,citecolor= MidnightBlue,filecolor=black,urlcolor= MidnightBlue} 

\title{Manuscript CS15-80\medskip\\
{\Large\em``The semantic drift of quotations in blogspace: a case study in short-term cultural evolution''}
\bigskip\\Response to Reviewers
}
\author{Sébastien Lerique and Camille Roth}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\makeatletter
\renewcommand{\subsection}{\clearpage\@startsection{section}{1}{0mm}
{\baselineskip}{\baselineskip\medskip\hrule\medskip}{\raggedright\Large\bf~\textcolor{MidnightBlue}}}
\makeatother


\newcommand{\critique}[1]{\begin{quote}#1\end{quote}}
\newcommand{\answer}[1]{{\setlength{\parindent}{0pt}\par\color{blue} #1}}
\newcommand{\tb}[1]{\textcolor{blue}{#1}}
\newcommand{\answerok}[1]{{\setlength{\parindent}{0pt}\par\color{MidnightBlue} #1}}
\newcommand{\unclear}[1]{\textcolor{red}{\sf [C: #1]}}

\vspace{3cm}
Dear Todd Gureckis,

\bigskip
We enclose an extensively revised version of the manuscript CS15-80 (initially entitled ``How do we copy and paste? The semantic drift of quotations in blogspace'') based on the report you sent us on Jul 18, 2015.

\medskip
We hope that we have appropriately addressed the detailed remarks and very constructive questions made by you and the referees: to this effect, we also provide below a detailed response to each of these comments.  We additionally used a different color in the manuscript to emphasize the portions which were thoroughly rewritten.

\medskip
This comprehensive revision work led to a significantly longer manuscript than was originally submitted --- from around 6,300 words to about 9,000 words now, making it closer to a ``regular article'' than a ``brief report'': we hope that this will not be problematic at this stage.

\medskip
We look forward to reading your feedback on this new manuscript and apologize in advance for this long revision round.

\bigskip
Best wishes,

\bigskip
Sébastien Lerique and Camille Roth

\subsection*{Editor}
\critique{
1. The main results of the paper are based on a lot of arbitrary decisions about how various quantities are defined.  I am interested if small differences in these choices radically changes the conclusions you would draw, and how many other approaches you tried before you arrived at the data patterns you present.  All the reviewers agreed on this point (asking for more clarification about feature selection, why certain choices were made in the analysis, etc…).  This ranges from what types of quotes were included, to the rules to detecting substitution errors, to the inclusion of certain features in your analyses.  The reader needs to be convinced that the few “strong” looking patterns you do find are not too dependent on these arbitrary choices.  I actually question if this is the best way to address this question (effect of memory and linguistic variables on copy errors) given the noisiness of the data and the difficulty in attributing copying errors to any particular parent
source.

}

\answerok{
We acknowledge these issues and worked extensively to address them.  In particular (further details may also be found in the responses to reviewers): 
\itx{
\x We developed a variety of substitution models to show that the results are robust
\x We rationalized feature selection, adding some variables and much detail to this section
\x In general, we clarified further the assumptions we made, such as the restriction to single substitutions
\x We also introduced manually-coded precision \& recall analyses for all filters, which justifies our choices.
}
}

\critique{
2. The conclusions about “contractile” processes seem pretty speculative (as almost all reviewers agreed).   Sure, this might be the theoretical implication of iterating this substitution process, but you do not present any further evidence of this type of contraction towards a fixed point.  I think if you want to make this a major conclusion of the paper there is more work to be done.  Also this idea has been explored, in my view, in a much more sophisticated way in the iterated learning literature (e.g., Kalish, Lewandowsky, Griffiths, Kirby… see my PDF comments for specific references).  You should really check these arguments and papers out because you really have an iterated learning task.
} 

\answerok{We agree that the initial manuscript overstated some results. We believe the new manuscript states its claims more appropriately, and makes a better distinction between actual conclusions and open questions that could not be addressed given the data (in particular, we attempted to explore substitution chains in earlier stages of this work, but were quickly limited by the constraints of the data).

We had also overlooked several developments in the iterated learning literature, and are grateful for the various pointers. The works we thought were most relevant have been integrated to the introductory sections, and their implications for our work are discussed in the new ``Discussion'' section.
}

\critique{3. The writing of this paper is of very low quality compared to what is expected at the journal.  I will email separately a very detailed editing of the paper via Adobe Acrobat where I tried to point these problems out.  However, it simply couldn’t be published without a nearly complete re-write or very heavy editing.
}

\answerok{Thank you for this very constructive and extensive feedback. We paid signification attention to the writing style in this new version, and large portions of the document have been rewritten and expanded --- all updates are marked in dark blue.}

%\critique{
%The bottom line is that while I think this is a really interesting idea/question, this is not the best execution of this idea that I could imagine being published in the journal.  My gut reaction is that this paper needs a lot of work, additional analysis, and rewriting before it could be published and therefore should be rejected.  However, because the reviewers are unanimously more positive in their evaluation I will invited a revision for this in the hope that you can make it a pretty significant change and can benefit from the excellent and detailed reviewer comments.
%
%So you understand what is requested:  typically papers which are revised at Cognitive Science are returned with a detailed cover letter responding to each of the reviewer comments in detail, along with pointers to what has changed in the paper.  I would appreciate this type of cover letter if you choose to resubmit because it can help me and the reviewers process what changes you made.
%
%Sincerely,
%Todd Gureckis
%Associate editor
%}
%
%\answer{
%}

\subsection*{Reviewer \#1}

\critique{I think this is an interesting approach and dataset for better understanding how sentences are recalled and how paraphrases are constructed.  However, I think that the experiment design needs to be refined, and the project as a whole needs to be better connected to larger theoretical problems and better grounded within cognitive science.}

\answerok{We have been careful to introduce many more connections to the relevant literature: the introduction has been entirely rewritten and is followed by a more thorough presentation of the related work in cognitive science, while the concluding sections (discussion \& concluding remarks) follow your recommendation of connecting our results with the literature.
}

\critique{
Starting with the grounding, the Introduction talks about theories of "knowledge transmission mechanisms" but doesn't provide examples aside from the quote-copying problem. The theories also don't re-appear in the results discussion, leaving the experimental results stranded and unconnected to any larger domain.  No discussion of sentence recall tasks within psycholinguistics is given - there isn't a huge literature on this, but there are at least a few papers that should inform this work, such as Potter \& Lombardi 1990 "Regeneration in the short-term recall of sentences". No discussion is made about whether and how much the substitutions change the quotes (e.g., is the substituted word a synonym, phonological-neighbor, or something else? - cf. Lauf et al 2013 "Analyzing Variation Patterns in Quotes Over Time"). Lastly, the meaning of the analysis (pg. 17-19) of this process as a dynamical system with "cultural attractors" is not fleshed out - why doesn't this predict that all quotes will eventually, through substitution, converge to the same string? (Does it?) As a result, the conclusion that psycholinguistic factors don't have a big impact on substitution likelihood is interesting but doesn't have clear theoretical repercussions, or any clear meaning outside the very specific problem of quote replication.  This needs to be substantially revised and expanded to make this work useful to the audience at large.
} 

\answerok{We thoroughly reworked the general grounding and flow of the manuscript. In particular:
\itx{
\x We believe that the introductory parts provide a clearer presentation of our question and how it relates to existing literature (including the areas we overlooked) and hypotheses
\x The new ``Discussion'' section, which was indeed missing, should properly reconnect our results to the related works introduced at the beginning of the paper.
}

Finally, several modifications in the ``Results'' section (more examples, a short discussion of the distance traveled by substitutions on the synonym network in footnote \#15, and the addition of the second null hypothesis $\mathcal{H}_{00}$ in the ``Variation'' subsection) should better describe the type of changes undergone by quotes.
}

\critique{ The experiment design also has some substantial flaws. First, some crucial factors were not considered - most notably, part of speech. I suspect, for instance, that pronouns and other closed-class words are less often substituted than open-class words.  Phonological neighborhood density could also be important, though probably less so.  Second, no analysis was done that included all of the factors in, e.g., a multi-feature regression model.  While the features being considered are not strongly correlated, they are still likely to interact - especially if part-of-speech is added.  Third, the restriction to single-word substitutions introduces some significant confounds.  I'm not convinced that correctly and incorrectly copying a quote should be viewed as separate behaviors.
}

\answerok{
We addressed all these points in the revised version:
\itx{
\x We checked and discussed the susceptibility of part-of-speech (POS). While \tb{a rigorous treatment of} the open-/closed-class question is largely precluded by our exclusion of stopwords from the analysis (which led us to not discuss that point in the manuscript), \tb{the data does indeed indicate that closed class words are less substituted than open class words (the \emph{Closed class-like} category is mainly made of \tb{closed class words})}. As the manuscript discusses however, this concerns less than 7\% of the substitutions we consider, and has likely no effect on our results.
\x Following this suggestion, we added phonological neighborhood density as a word feature. We show that it forms a subgroup of strongly correlated variables, together with orthographic neighborhood density, and choose the latter as a more natural representative of this subgroup given the task studied.
\x We introduced a multi-feature regression model to estimate variation upon substitution, showing that the magnitude of the variation of a feature is mainly predicted by the {start word's value for} that very feature, generally with negligible interaction with the other features (see also below and figure 8).
\x We expanded substitution models to {also detect quotes undergoing up to two substitutions}, which yielded the same results as single substitutions {(see below)}. 
}

%We also examined the effect of part-of-speech (POS): Figure 5 details the results on five POS categories. A Goodman-based multinomial goodness-of-fit shows that POS categories do not capture any significant bias in the selection of substitution targets, which is why we largely keep them outside of our analysis.
}

\critique{But more importantly, the $\sigma_\phi(f)$ calculation can be misleading under the single-substitution restriction: Consider a toy case where all quotes are two words, and there is a single binary feature $F$. $p(F=1) = 3/4$, and the true substitution probabilities, given the value of F, are $p(S|F=1)=1$, $p(S|F=0)=1/2$.  All the quotes where both words have feature $F=1$ will have two substitutions and be omitted from the dataset, as will half of the quotes with one $F=1$ \& one $F=0$ word.  Half of the quotes with two $F=0$ words will also be omitted for having no substitutions.  Due to these omissions, if you actually generate some test data and calculate the $\sigma_F$ values, the $\sigma_F(1)$ value is correctly estimated as 1, but the $\sigma_F(0)$ value is erroneously estimated as 1/8 (not 1/2), because the $F=0$ words are never substituted in the $F=1/F=0$ pairs, and these are three times more common than the $F=0/F=0$ pairs.  I haven't run more complicated cases, such where the length of the quotes vary, but this leads me to have limited confidence that the estimates in the Figure 4 are appropriate to draw conclusions from. I would want to see either a mathematical analysis with realistic values showing that the error is expected to be small in this data or an expansion to include multiple-substitution examples to avoid this problem.
}

\answerok{The case presented here is valid and could indeed hide a bias in our measures.
Rather than analyzing all possible such cases to bound the error, we extended substitution models to detect quotes undergoing up to two substitutions, and applied the same analysis to these mined substitutions. The results were unchanged (note: graphs produced for all models may be found at <https://huit.re/aEH77Gc3>, and may be commented further upon request). For example, the extended version of the model presented in the paper mines 70k substitutions, 9.2k of which are kept after filtering (the lowered acceptance rate indicates that more invalid substitutions are generated when allowing up to two substitutions per quote).

We did not expand to more than two substitutions, as more than 20\% of quotes are 5 to 9 words long (with a minimum of 5 words for quotes in the data set). For such quotes, three or more substitutions means changing at least a third of the sentence, which makes any attempt to link it to a similar variant pointless.
}

\critique{ Some high-level suggestions:
\itx{
\x more discussion of what dynamical systems are and especially their interpretation in this task before their appearance in the results}
}

\answerok{We clarified the notion of dynamical systems when rewriting the introduction and in the ``Variation'' section (including footnote \#21).}

\critique{\itx{
\x needs a substantive discussion of the results}
}

\answerok{An entirely new ``Discussion'' section is now devoted to this task.}

\critique{\itx{
\x show that the phi value is capturing what it purports to capture
\x and that the factors aren't conflated with length
\x is the single-substitution biasing the data?
\x make sure that the reported measures are interpretable
\x if low-frequency words have 1/3 chance of being chosen as substitution (Fig. 4), what if there are four low-frequency words in a quote?
}
}

\answerok{\tb{The measure we had proposed in the first version of the manuscript (for information, this was the subject of Fig. 4 in that manuscript) considered probabilities of substitutions as an aggregate quantity (and not at the individual sentence level).  It was difficult to use it to understand what happens when substitution occurs in a given sentence.}

\tb{We therefore reworked the notion of susceptibility to make it more straightforward}. We now define it relatively to a null hypothesis: it measures the strength of a bias affecting a given feature value (i.e., how much a feature value is substituted compared to what it would be if targets were picked randomly). We present the corresponding results in Figs. 5 \& 6.

Finally, we also extended our measures to capture part of the sentence-level context in which substitutions occur (see the ``Sentence context'' subsection). Sentence-relative susceptibility, in particular, gives a straightforward answer to the question of which words are more likely to attract substitution within a given sentence.
}

\critique{\itx{
\x improving statements of motivation and grounding within cognitive science
\x what are representation transformation processes?
\x give examples in the introduction
\x show how the quotation test case connects to more general representation transformation?}
}

\answerok{
These points should be addressed by the improved introduction. In particular regarding the last question, the new ``Discussion'' section additionally makes a connection with the iterated learning literature.
}

\critique{\itx{
\x what about the semantics of quote replication?
\x does part of speech influence substitution rate?
\x can you quantify the semantic distance between the original and substituted quote (e.g., get Mechanical Turkers to rate the semantic distance?)}
}

\answerok{We added a short discussion on the semantics of substituted quotes. We first aimed at quantifying the semantic distance by estimating the distance ``traveled'' by the substituted word in the WordNet hypernym-hyponym network as well as the FA network. This showed that substituted and substituting words are rarely direct synonyms, yet they usually achieve a similar meaning in the context of their sentence.
To further grasp the relationship of substitutions to synonyms we introduced a second null hypothesis in the ``Variation'' section, which compares the actual variation to what would happen if a random synonym of the start word was picked instead.

Finally, we also examined the effect of part-of-speech (POS): Fig. 5 details the results on five POS categories, \tb{with a discussion of the corresponding effects in the main text, explaining} why we largely keep POS outside of our analysis.
}

\critique{\itx{
\x what happens when you run a multi-factor regression model? Do the features interact?
}}

\answerok{Thank you for this question. We introduced a simple regression model to check for feature interactions, which is presented in Fig. 8 with a corresponding discussion (in a nutshell: we show that features generally do not interact).}

\critique{\itx{
\x clarify from the beginning the predictions you are making so that the results will be interpretable 
}}

\answerok{We hope the improved introduction is clearer as to what we expect and what is left open due to the exploratory nature of our study.}

\critique{Some low-level points:\itx{
\x the in vitro/in vivo distinction is unfamiliar and ought to be defined in the text
}
}

\answerok{In light of this potential unfamiliarity, we preferred to entirely avoid referring explicitly to this dichotomy (for instance, we now speak of ``corpus-based utterances'' instead of ``in vivo'').}

\critique{\itx{
\x introducing eight measures and then removing three of them due to collinearity is presentationally confusing}}

\answerok{We agree that introducing twelve\footnote{In the revised version we have 12 measures, instead of 8 in the initial submission.} measures before eventually keeping six of them could complicate the flow of the section on word-level measures. However, we found it more natural to first propose a comprehensive list of candidate features based on the literature before addressing the quantitative analysis with a reduced yet sufficient set of metrics --- especially now that this section has been significantly expanded. We initially favored this solution over first presenting the final list of candidate features and justifying afterwards that they are anyway correlated with a broader list of traditional variables. (Note: An alternative option could consist in presenting the contents of Figs. 1 and 2 in the same figure, if this would be less confusing.)}

\critique{
\itx
{
\x was there a reason to expect that \# of synonyms and \# of phonemes would interact (Fig. 4)?]
}
}

\answerok{Except for word frequency, we observed that synonyms and phonemes were the only two word features to exhibit an effect on susceptibility (and a quite similar effect for that matter). We thus reasoned that they might exhibit a joint effect.

However, the revised version now features a much broader framework to statistically assess joint effects (see Fig. 8); we think it favorably replaces the previous variable-to-variable correlation check. (Note: we also replaced the number of phonemes with the number of letters -- which makes more sense in our context, see the end of the ``Word-level features'' subsection --, which does not seem to interact with number of synonyms).}

\critique{\itx{
\x how are you binning the continuous-valued features into categorical features for Fig 4 \& 5?
\x are there multiple y=x crossovers in Fig 5 if more bins are used?}
}

\answerok{Data in Figs. 4 \& 5 was binned into 5 integer-based bins, and the graphs were cropped whenever the data was unreasonably scarce and, accordingly, confidence intervals were unreasonably large (this cropping gave the impression of some graphs having only 4 bins).

Notwithstanding, data in the new Figs. 6, 7, 9 \& 10 is now binned into 4 quartile-based bins, which makes more sense for the confidence intervals. This decision was reached after exploring several other binning configurations: none of these substantially changed the results, but opting for quartile-based bins does improve the overall readability. Notably, no multiple crossovers were ever observed.
}

\subsection*{Reviewer \#2}
\critique{This is a review of "How do we copy and paste? The semantic drift of quotations in blogspace." This paper attempts to discover the underlying psycholinguistic variables that is involved in blog authors making mistakes in the copying of quotations across time. Although I think the motivations and goals of this article are interesting and noteworthy, I think the actual analysis needs improvement in a number of areas.

Although I appreciate the brief writeup on intrusions in recall, the article fails to capture the amount of research done on this topic. Going back to Deese (1959; the precursor to modern experimental studies on false memory) and the subjective organization results of Tulving (1962) it has been well known that extra-list intrusions in free recall are often semantic in nature, with the intruding word being very similar in meaning to a studied item. A recent study that reinforces this is one by Zaromb, et al. (2006; JEP:LMC).
}

\answerok{Thank you for these very useful and, indeed, relevant pointers.  We accordingly devoted a significant portion of the literature review to the issue of recall (see the ``Related work'' section).}

\critique{ This leads into a larger issue: there is no direct word-word semantic similarity score included in the psycholinguistic variables in the analysis. Global measures such as clustering coefficients based on free association norms are included, but this isn't as appropriate, since as the authors find, the vast majority of words that are being substituted are low frequency words. Low frequency words are not as well represented in the Nelson norms, given that they are less likely to be produced, and are thus less likely to have an appropriate clustering coefficient from this dataset. Variables based on corpus analyses need to be included to provide an appropriate overview of the impacts of semantics on this task. Even so, I would suspect that global semantic network measures are not nearly as important as word-word metrics.

I would guess that the substitutions that are made in quotations are overwhelmingly higher frequency, highly semantically similar words. As the example on pg. 19 states, the substitution was "world" for "globe," where a higher frequency word replaced a lower frequency, semantically related word. There are a number of different models that can be used to assess this type of similarity, with the classic model being latent semantic analysis (Landauer \& Dumais, 1997). There are a number of additional models that have been proposed, such as Topics (Griffiths, Steyvers, \& Tenenbaum, 2007, Psyc Review) or Beagle (Jones \& Mewhort, 2007, Psyc Review). These models differ in their implementational complexity, but there are tools available that allow for useful statistics to be acquired easily. A tool by Recchia \& Jones (2010, BRM) allows for pointwise mutual information (PMI) of different words to be retrieved from a given corpus very simply. A tool such as this would allow for word-word similarity to be included in the analysis.

I understand that word-word metrics are not easily introduced in the type of analysis that the authors are attempting to do, where the analysis of the contributions of different global psycholinguistic variables are being weighed against each other. I just think that type of analysis is inappropriate here. Since the quotations were selected on the basis of single substitutions, it seems to me that the appropriate level of analysis is what properties of the word that is being substituted is the most predictive of the substitution. I suspect that the combination of frequency and semantic similarity will be by far the most predictive variables, consistent with past results on recall.
}

\answerok{The rewritten introductory sections of this revision now aim at explaining why we chose to use a feature-based analysis over the prediction of individual words (see below for more details about this).
Word-word similarity is now also shortly discussed in the beginning of the ``Results'' section (footnote \#15). We explored a number of other similarity measures provided by WordNet (based on paths in the WordNet network, or on similarity of Information Content extracted from external corpora), but none of them provided more insight than what is being discussed in that section.

\tb{Regarding prediction, we would like to distinguish two questions. First, prediction of the actual disappearing and appearing words, which would be an extremely interesting result, but a prospect we believe to stand several steps away from what is currently possible.
As the revised introductory parts now allude to, the prediction of word intrusions is well mastered in the case of manually designed word lists but is not as exact in the case of random lists of words. Zaromb et al. (2006), for instance, predict the list from which prior-list intrusions come, but not the exact intruding word. To our knowledge, the prediction of intrusions in full sentences (either manually designed or not) has not been tackled. Finally, as explained in footnote \#3 and the ``Discussion'' section, our data set is ill-shaped to measure the accuracy of predictions we could make.}

\tb{These reasons led us to focus on the second question: prediction of properties of the disappearing and appearing words. Indeed, Figs. 7 \& 10 provide average predictions of appearing word features given the properties of a disappearing word. We show that there are little to no interactions between word features (see Fig. 8), such that word frequency is not sufficient to predict variation of other features. Finally, we introduced a new null hypothesis ($\mathcal{H}_{00}$) in the ``Variation'' subsection which gives insight into how an appearing word relates to the synonyms of the word it replaces, and shows that appearing words are much more frequent than the average frequency of synonyms of the disappearing word.}

While we understand that the analysis we present differs from typical approaches usually taken for similar laboratory data, we hope that these various additions contribute to alleviate concerns regarding word-word similarity.
}

%\answer{We agree that predicting the actual disappearing and appearing words would be an extremely interesting result, \tb{yet we believe that this prospect stands} several steps away from what is currently possible.
%\tb{As the revised introductory parts now allude to, the prediction of word intrusions is well mastered in the case of manually designed word lists but is not as exact in the case of random lists of words. Zaromb et al. (2006), for instance, predict the list from which prior-list intrusions come, but not the exact intruding word. To our knowledge, the prediction of intrusions in full sentences (either manually designed or not) has not been tackled. Finally, as explained in footnote \#3 and the ``Discussion'' section, our data set is ill-shaped to measure the accuracy of predictions we could make.
%}}

\critique{Beyond this, I think the authors are missing out on the most interesting aspect of the data that they have collected: how the surrounding context of the quotation used is changing across time. That is, the quote is being propagated, and the prior usage of that quote is likely predictive of how that information is spread. This is likely a dynamical process where multiple news sources are used to construct a new blog post, and being able to analyze this type of information propagation would be a very interesting study. I point to the semantic space models cited (and the work derived from them) as being very useful tools to analyze this type of question.
}

\answerok{This would indeed be a very interesting project and prospect, but the data we collected do not enable the study of the surrounding context (in particular, we know neither the prior usage nor the way information sources may be used) so that this goal remains beyond the scope of our paper.

We did however extend our measures to capture part of the context of the sentence substitutions occur in (see the ``Sentence context'' subsection).}

\subsection*{Reviewer \#3}
\critique{In this paper, the authors analyze a large data set of quotations taken from a variety of news sources to study the form of substitutions in quotes, to determine if the transformations follow expected patterns given psycholinguistic theory.  Specifically, the words that are likely to be substituted are found to be less frequent words, and the pattern of substitution is such that the substituting word is more frequent, acquired earlier, and has lower clustering in the free-association word network.  These patterns are consistent with expected cognitive biases.

There are a few issues I would want to see addressed.
\bigskip

Foremost is the method of detecting substitution.  I understand that, given the data set, there is no perfect way to identify what quote is a transformation of what quote, and that it is not feasible to test all possible methods AND that it is simpler to focus on one method for the purposes of the paper.  However, given that this decision of which method to use can directly influence the main conclusions, it behooves the authors to do some sort of robustness check around this method.  For example, one could imagine two distinct groups that only exactly copy from themselves and never copy from each other, where one always publishes earlier than the other.  If the earlier-publishing group tended to select variations of quotes that used lower-frequency words, and the later-publishing group tended to select variations of quotes that used higher-frequency words, this method would infer many more low-frequency to high-frequency substitutions than actually existed.  This may not be a likely scenario, but it demonstrates the benefit of a robustness check. Trying the same analysis with one or two different substitution models would strengthen the authors' claims.
}

\answerok{There are indeed very many possible models, yet we agree that a robustness check is also very sensible.  Substitution detection relies on a number of assumptions and we identified four binary parameters that differentiate potential models, such that the resulting 16 combinations cover most of the reasonable answers to inference uncertainties. We thus implemented, tested and checked these models, and we can report that our results remain valid for all 16 models (for the sake of clarity, we do not specifically report on these models, though).  \\

This leads to a much more detailed ``Substitution Model'' subsection which discusses these options along with the reference model we used throughout the paper.
}

\critique{ A second issue is the time spent on feature selection.  It is unclear why the authors do feature selection at all, as "the analysis is done on a per-feature basis."  It would again add to the robustness of their claims if they showed that all of the correlated features show the same pattern of susceptibility and bias in substitution.  In the same vein, it is not clear why the authors claim that age of acquisition, frequency, and number of synonyms have low levels of correlation "excluding the network properties".  Why would one ignore the correlations with the network features?
}

\answerok{Our feature selection step aims at making the results more intelligible. This section was indeed poorly written, and has been thoroughly revised to rationalize the process. In particular, we identify (six) groups of strongly correlated features (both manually by visual inspection and automatically with the help of an agglomeration algorithm, yielding identical results) and justify the choice of our six prototype features for each group.  Features belonging to the same group generally exhibit very similar behaviors across all measures (with few exceptions, notably number of phonemes and number of syllables \tb{exhibiting less cognitive bias} than number of letters). Graphs produced for all features may be found at <https://huit.re/aEH77Gc3>, and could be included in a Supplementary Information bundle if requested.
}

\critique{
One smaller issue has to do with the filtering of the quotations.  The authors manually inspected the filtered quotes to see how many real event-related utterances are incorrectly filtered, but I would argue the precision of the filter is more important than the recall; how many of the kept quotes are not real event-related utterances?  Why not do manual coding of those and report that accuracy?
} 

\answerok{This is an important empirical point which we did not address systematically in the first submission.  We have now done this manual coding step, and improved filters wherever precision or recall were not good enough. We now assess filtering accuracy systematically and it is satisfying in the relevant cases.}

\critique{Finally, while I understand the appeal of doing a dynamic systems analysis to find the convergence points of the substitutions, but it is not really valid here.  The analysis in this paper does not provide information on what happens with multiple substitutions, so the authors cannot really make claims about the convergence points given this methodology.
}

\answerok{In this revised version, we better explained our claims and their limits (see for instance the second paragraph of the ``Discussion'' section), without overstating our findings.}

\critique{Overall, the analysis of the patterns of substitution in a series of quotations to validate psycholinguistic theory is novel and advances the current science.  The results would be more solid with some additional robustness checks, but in general they are quite intuitive.  I hope in the future the authors find a dataset that provides certain information about the transmission of language from one person to another, as this would allow for stronger claims about causality and greater confidence in the pattern of substitutions, as well as a finer-grained analysis of repeated substitutions.
}

\answerok{On a side note, we entirely agree with the relevance of pursuing this research program in these directions, as the \emph{in vitro} observation of information transmission and its evolution is part of an ongoing project (which nonetheless remains beyond the scope of the current paper).}

\subsection*{Final remarks}

\answerok{A few additional numbers have changed in this revised manuscript:
\itx{
\x Clustering coefficient values have changed, because Free Association link weights are now taken into account in the computation (i.e. coefficients are computed on the undirected weighted graph instead of the undirected unweighted graph).
\x An update in the language detection module led to a small change in the results of cluster filtering: there are a few more clusters and quotes than in the previous version. As a result of this, the number of words coded by Word Frequency has also changed (since frequency of words is computed from the set of quotes we keep after cluster filtering).
\x The improvement of substitution filtering led us to mine many more substitutions than previously. All in all raw data processing is now much more reliable, as unit tests cover nearly all of it.
}
}

{\small
\bibliographystyle{../../authordate1}
\bibliography{../../mainbiblio-u8}
}
 
\end{document}
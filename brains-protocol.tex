% !TEX root = brainscopypaste.tex
% ============================
\section{Protocol} % =========
% ============================
\label{sec:protocol}

In order to start bridging this gap, we set out to \emph{empirically} study public representation transformations at the microscopic level, aiming to stay compatible with macroscopic-level studies of these public representations.
Quotations appeared to be a perfect candidate as public representations.
First, they are usually cleanly delimited by quotation marks (and often with HTML markup in web pages), which greatly facilitates their detection in text corpora.
Second, they stem from a unique ``original'' version, and could ideally be traceable back to that version.
Third, and most importantly, their duplication should \emph{a priori} be highly faithful, apart from cases of cropping: not only should transformations be of moderate magnitude, but when specific words are not perfectly duplicated, it is safe to assume that the variation is due to involuntary cognitive bias --- as writers may expect any casual reader to easily verify, and thus criticize, the fidelity to the original quotation.
Quotation evolution is therefore a perfect environment to measure cognition-induced transformations and relate those findings to macroscopic social dynamics.

\subsection{Dataset}

We relied on a reliable quotation dataset collected by \citet{Leskovec09}, large enough to lend itself to statistical analysis.
This dataset consists of the daily crawling of news stories and blog posts from around a million online sources, with an approximate publication rate of 900k texts per day, over a nine-month period of time (from August 2008 to April 2009) \cite{Leskovec09-url}.\footnote{Unfortunately, the original article~\citep{Leskovec09} does not provide additional details on the source selection methodology.}
Quotations were then automatically extracted from this corpus: each quotation is a more or less faithful excerpt of an utterance (oral or written) by the quoted person.
Quotations were then gathered in a graph and connected according to their similarity: either because they differ by very few words (in that case, no more than one word) or because they share a certain sequence of words (in that case, at least ten consecutive words).
A community detection algorithm was applied to that quotation graph to detect aggregates of tightly connected, i.e. sufficiently similar, groups of quotations (see \citet{Leskovec09} for more detail).
This analysis yielded the final data we had access to, with a total of about \num{70000} sets of quotations; each of these sets allegedly contains all variations of a same parent utterance, along with their respective publication URLs and timestamps.

\subsection{Word-level measures}

To keep the analysis palatable, we restricted the analysis to quotation transformations which consisted in the \emph{substitution} of a word by another word (and only those cases).
To quantify those substitutions, we decided to associate a number of features to each word, the variation of which we can statistically study.
The following sections detail the features we used.

\subsubsection{Standard psycholinguistic indices}

We first introduce some of the most classical psycholinguistic measures on words:

\rk{Add some bibliography about those features' known effects}

\begin{itemize}
    \item \textbf{Age of Acquisition}: the average age at which words are learned, obtained from~\citet{kuperman12},
    \item The average \textbf{Number of Phonemes} for all pronunciations of a word, obtained from the Carnegie Mellon University Pronouncing Dictionary~\citep{Weide98},\footnote{The CMU Pronouncing Dictionary is included in the NTLK package~\citep{Bird09}, the natural language processing toolkit we used for the analysis.}
    \item The average \textbf{Number of Syllables} for all pronunciations of a word, also obtained from the CMU Pronouncing Dictionary.
    \item \rk{Why exactly don't we include word frequencies as a measure? The alleged self-correlation loop doesn't determine at which end of a substitution a given high-frequency word would be, so it doesn't really force correlation}
\end{itemize}

We also considered grammatical types within quotations by detection of \emph{Part-of-Speech} (POS) categories, using the Penn TreeBank Project typology~\citep{Santorini90} and thereby distinguishing verbs, nouns, adjectives and adverbs.
The results were however extremely similar across the various categories, exhibiting no specific effect of words belonging to different POS categories.

\subsubsection{Network-based measures}

Additional to classical psycholinguistic measures, we also considered more recently studied variables based on semantic network properties.
We relied on two types of network-based data: curated synonym associations provided by WordNet~\citep{WordNet10}, and ``free association'' norms collected by~\citet{Nelson04}.

The WordNet (WN) database gathers more than \num{117000} concepts and about \num{147000} words attached to these concepts.
WordNet entries are disambiguated words, called ``lemmas'', which are grouped into concepts called ``synsets'': all the lemmas in a synset are synonyms for the same single meaning.
Note that synsets also encode grammatical category, so the choice of a synset for a word (thus creating a lemma) immediately determines the grammatical category of that word.
Non-disambiguated words make the nodes of the semantic network we build (i.e. homograph lemmas are aggregated into a single node), and a node's neighbours are synonymous lemmas belonging to the same synset(s) as the lemma(s) represented by that node.
In other words, synsets induce cliques in the semantic network, and a word with several meanings belongs to as many cliques as it has meanings.
\TB{Links connecting two synonymous lemmas are assigned a weight corresponding to the number of times these lemmas are mentioned by WordNet as synonyms, i.e. in the same synset.}
\rk{We might spare ourselves that last sentence since we drop that aspect further down.}

Free Association (FA) norms record the words that come to mind when people are presented with a given cue (that is the ``free association'' task).
As \citeauthor{Nelson04} explain,
\begin{quote}
free association response probabilities index the likelihood that one word can cue another word to come to mind with minimal contextual constraints in effect.~\citep{Nelson04}
\end{quote}
Following \citet{Griffiths07}, we consider the directed weighed network formed by the association norms, that is the network where words are nodes and edges are directed from cue to associated word, with a weight equal to the probability of that target word being produced when this particular cue was presented.

While the original WN network is a weighted, undirected network, the FA network is a directed network where weights have a different meaning.
Both semantic networks describe synonymy relationships corresponding to qualitatively distinct forms of association: either from a linguistic viewpoint (WN) or from the perspective of actual information retrieval by humans (FA).
In  order to work in a unified framework, we shall consider links as an information on the existence of some sort of association between two words and transform both networks into undirected, unweighed networks.
\rk{Check that's the case in the analysis}

\bigskip
We introduce three standard network-based measures to be used on both the WN and FA networks:

\begin{itemize}
    \item \textbf{Distance} between two words, a classical dyadic network measure corresponding to the shortest path length between two nodes.
    This measure may be used as a proxy of the remoteness between two given words, which is likely to influence the likelihood of a word being a relevant substitution candidate for another word.
    \item \textbf{Centrality} $k$, measured by the number of neighbours of a given node, which can be interpreted as a proxy for the polysemy of words.
    \TB{For the directed network FA we also define the incoming centrality $k'$ (number of neighbours pointing \emph{to} that node)}.
    \rk{Remove that, since we restrict ourselves to undirected networks}
    Note that in the present case, there is a quasi-perfect correlation between node degree and node \emph{PageRank}~\citep{Page99}, which had already been used in~\citet{Griffiths07} and may be interpreted as a generalized and recursive measure of word polysemy: central nodes in the PageRank sense are those which are linked to many nodes themselves linked to many nodes, and so on recursively.
    \item \textbf{Clustering coefficient} $c$, which measures the extent to which a node belongs to a local aggregate of tightly connected nodes, and defined as the ratio between the number of actual v. possible edges between a node's neighbours \cite{watt-coll}.
    In the WN network, a word is likely to have a higher clustering coefficient whenever the synsets to which it belongs are themselves semantically linked.
    This measure had been used by \citet{Chan10}.
\end{itemize}

\TB{Betweenness coefficient, another measure of node centrality describing the extent to which a node tends to connect otherwise remote areas of the network~\citep{free:set}.
More technically, it corresponds to the normalized number of shortest paths connecting dyads which pass through that node; the higher the coefficient, the more important that node is in ensuring the connectedness of the rest of the network.
Correlation between FA-BC and FA-PR is $0.74$; between WN-BC and WN-PR it is $0.83$.}
\rk{We dropped this, right?}

\subsubsection{Variable correlations}

An important question here relates to the various possible correlations between all the variables we consider.

The age of acquisition is a key variable, primarily as a usual suspect in psycholinguistic studies, and also appears to be generally correlated to many of the other variables.
This relates to an ongoing debate suggesting that age of acquisition encodes a variety of phenomena which are difficult to disentangle from more specific phenomena, which could be captured by more independent variables such as \CN.
\rk{What do we conclude of that? Nuance results with AoA?}
\TB{Indeed, the minimal linear correlation of $aoa$ ($-0.31$) with other variables has a higher magnitude than the maximal linear correlation of other variables with each other ($-0.26$).}
\rk{how do we get to that conclusion?}

Additionally, number of phonemes and number of syllables exhibit a strong linear correlation ($0.85$).
The analysis showed a better prediction effect of number of phonemes over number of syllables, which is consistent with~\citet{nick-diss}, and we therefore chose to focus the presented results on the former only.
\rk{Check that!}

\bigskip
Network properties, finally, appear to be relatively weakly correlated with each other, across one network or between both networks (i.e. nodes with high index values are generally distinct between FA and WN).
We chose to focus the results on FA as this network presents a direct psychological interpretation (that human copy-pasting may or may not show the same type of trend as human free association), noting that prediction results are matched against WN as well.
\rk{comment peut-on dire que les nœuds importants dans FA ne sont pas nécessairement importants dans WN mais que, pourtant, la forme de la prédiction est la même? cela me semble illogique en première lecture}
\rk{Check those assertions}

\begin{figure}[!th]
    \includegraphics[width=\linewidth]{algorithms/Rplot.pdf}
    \caption{Variable correlations \rk{rebuild with the variables we talk about and keep}}
\end{figure}

\subsection{Detection of substitutions}

The data we use presents an additional challenge: each set of quotations bears no explicit information either about the authoritative original quotation, or about the source quotation(s) each author inspired himself from when creating a new post and reproducing (possibly altering) those sources.
Substitutions are therefore not explicitly encoded in the dataset.

We face a reverse engineering problem where, given all quotations and their occurrence timestamps, we must estimate which was the originating quotation for each instance of each quotation.
We therefore model the underlying quotation selection process by making a few additional assumptions which let us define substitutions from the available data.

The main question in this problem concerns the way we consider later occurrences of a quotation which, when it first appeared, was identified to be an alteration from an original quotation.
Let us give an example: say the quotation ``These accusations are false and absurd'' ($q_a$) appears in a blog on January 18, and the slightly different quotation ``These accusations are false and incoherent'' ($q_b$) appears in another blog on the 19th, the 20th, and the 21st of January.
If $q_a$ was sufficiently prominent when $q_b$ first appeared, we can safely assume that the author of $q_b$ on the 19th based himself on $q_a$.
But what about the following occurrences of $q_b$?
Should we consider them to be substitutions based on $q_a$ (i.e. re-creations of $q_b$ by a new instance of the substitution process that brought from $q_a$ to $q_b$ in the first place) or reproductions of the first occurrence of $q_b$?

To settle this question we model the process as follows: we assume that when a quotation $q$ appears at time $t$, if it is not original (i.e. if not stemming from a source external to the dataset, e.g. initiating a new set of quotations), then it is based solely on the most frequent quotation $q_{max}$ in the preceding period of time $[t - \Delta t ; t]$.
The length $\Delta t$ of that period of time is fixed as a fraction of the total duration of the set of quotations; we took one fifth in the implementation (i.e. one month for a set of quotations spanning five months, or one day for a set spanning only five days; this takes the dynamism of the set of quotations into account).
If $q$ differs from $q_{max}$ by only a word, it is counted as a substitution from $q_{max}$ to $q$.
In any other case, i.e. if $q$ and $q_{max}$ are the same or if $q$ and $q_{max}$ are different in other ways, the occurrence of $q$ is not considered to be an instance of substitution and is discarded.

An example of this detection in a situation akin to the one described above can be seen on figure \ref{fig:model-slidetimebags}: $q_a$ and $q_b$ differ by only a word, and $q_a$ appears first and stays the most frequent in the beginning.
This is why the occurrences of $q_b$ at $t_1$ and $t_2$ are detected as substitutions stemming from $q_a$.
After a time the situation is reversed: $q_b$ becomes more frequent than $q_a$.
This entails that occurrences of $q_b$ are seen as reproductions of itself ($t_3$), and that occurrences of $q_a$ are detected as substitutions stemming from $q_b$ ($t_4$), i.e. \emph{re-creations} of $q_a$.

\begin{figure*}[h]
    \centering
    \def\svgwidth{\textwidth}
    \small
    \input{images/model-slidetimebags-b.pdf_tex}
    \caption{Detection of substitutions according to model \rk{update text in figure}}
    \label{fig:model-slidetimebags}
\end{figure*}

The assumptions embedded in this model are only a subset of a wider set of possibilities, each leading to alternative models.
We identified and implemented five other such models, and they all yielded essentially the same results.
These models differ in the definition of the source quote from which new occurrences stem, essentially modifying the balance between reproduction of previous occurrences of a quote and re-creation of itself by a new substitution instance.
For example, the time-windows considered can have different lengths, can include all occurrences from the beginning of the set of quotations, or can have fixed positions.
The source quotation of a potential substitution can be chosen among those time-windows, or otherwise (e.g. among \emph{all} quotations having appeared before $t$ and differing by a word from the arrival quotation; this detection process would include all possible substitutions detected by other models, but would also include many false positives).

\subsection{Characterization of substitutions}

We then measure the alterations introduced when authors reproduce quotations by comparing the relative features of substituted and substituting words.
Note that since we only consider substitutions, we measure the features of an alteration \emph{knowing that there has been a alteration}, and we do not take invariant quotations into account.
Indeed, in the first case we know there has been a human reformulation, whereas in the second case it is impossible to know whether there has been perfect human reformulation or simply digital copy-pasting of a source (``{\sc Ctrl-C}/{\sc Ctrl-V}'').

We build two main observables for each word feature.
First, we measure the variation of word feature over a substitution, looking at the variation of a given feature between arrival and start words.
Second, we measure the susceptibility for words to be the source of a substitution, knowing that there has been a variation, in order to show which semantic features are the most likely to attract a substitution.

\subsubsection{Alteration}

For a given feature $\mathfrak{F}$, we measure how a word $\wstart$'s feature varies as $\wstart$ is substituted by $\warrival$.
Let us denote this quantity:
$$\Delta (\wstart,\warrival) \defeq \mathfrak{F}(\warrival) - \mathfrak{F}(\wstart)$$

Averaging this value over all start words with a given feature value $f$ yields the mean variation for that feature value $f$.
\footnote{To avoid any auto-correlation effect due to the number of substitutions in a cluster (possibly leading to an overly optimistic estimation of confidence intervals), we first average substitutions over each cluster, by considering the average of arrival word features for a given start word.
Indeed, substitutions occurring in the same cluster are likely not statistically independent.}
This quantity can be written:
$$\left< \Delta(\wstart,\warrival) \right>_f = \left< \mathfrak{F}(\warrival) - \mathfrak{F}(\wstart) \right>_{\left\lbrace (\wstart,\warrival) | \mathfrak{F}(\wstart) = f \right\rbrace}$$

We introduce a null hypothesis $\mathcal{H}_0$ to compare the actual variation of a word's feature to its expected variation, assuming the arrival word $\warrival_{\mathcal{H}_0}$ had been chosen randomly from the pool of free association words.
\rk{Check that's true in the code}
The corresponding average quantity over all start words may be written:
\footnote{Note that $\warrival_{\mathcal{H}_0}$ is in fact constant in this averaging, since by definition it does not depend on $\wstart$.}
$$\left< \Delta(\wstart,\warrival_{\mathcal{H}_0}) \right>_f = \left< \mathfrak{F}({\warrival_{\mathcal{H}_0}}) - \mathfrak{F}(\wstart) \right>_{\left\lbrace (\wstart,\warrival_{\mathcal{H}_0}) | \mathfrak{F}(\wstart) = f \right\rbrace}$$

We also considered an alternative null hypothesis, denoted $\mathcal{H}_{00}$, where the arrival word is chosen randomly \emph{among immediate synonyms of the start word} (neighbours in the WN network \rk{We could also do that with neighbours in directed or undirected FA}), i.e. an arrival word chosen among semantically plausible though still random words.
\footnote{In this case $\warrival_{\mathcal{H}_{00}}$ does depend on $\wstart$.}

Using this method we obtain the mean variation of feature for each start feature value, and can compare the variations to a situation where arrival words are chosen randomly.
This gives us a fine-grained view of how word features evolve upon substitution.

\subsubsection{Susceptibility}

Furthermore, the protocol lets us compute substitution \emph{susceptibilities} for each feature value $f$.
We say that a word is \emph{substitutable} if it appears in a quote which undergoes a substitution, whether that substitution operates on the considered word or on another.
Word substitution susceptibility (denoted $\mathfrak{S}_{\mathfrak{F}}(w)$) is computed as the ratio of the number of times $n_s(w)$ a word is substituted to the number of times $n_p(w)$ that word appears in a substitutable position.
We have:
$$\mathfrak{S}_{\mathfrak{F}}(w) \defeq \frac{n_s(w)}{n_p(w)}$$

Now averaging over all words having a given feature value $f$, we obtain the mean susceptibility for the feature value $f$:
$$\left< \mathfrak{S}_{\mathfrak{F}} \right>_f = \left< \frac{n_s(w)}{n_p(w)} \right>_{\left\lbrace w | \mathfrak{F}(w) = f \right\rbrace}$$

This measure focuses on the selection of start word instead of the selection of the arrival word.
Indeed, the features have an effect not only on the choice of a new word when a substitution takes place, but also at the preceding moment when it is not yet known which word in the quotation -- if any -- will be substituted.
% !TEX root = brainscopypaste.tex
% ============================
\section{Protocol} % =========
% ============================
\label{sec:protocol}

In order to start bridging this gap, we set out to \emph{empirically} study public representation transformations at the microscopic level, aiming to stay compatible with macroscopic-level studies of these public representations.
Quotations appeared to be a perfect candidate as public representations.
First, they are usually cleanly delimited by quotation marks (and often with HTML markup in web pages), which greatly facilitates their detection in text corpora.
Second, they stem from a unique ``original'' version, and could ideally be traceable back to that version.
Third, and most importantly, their duplication should \emph{a priori} be highly faithful, apart from cases of cropping: not only should transformations be of moderate magnitude, but when specific words are not perfectly duplicated, it is safe to assume that the variation is due to involuntary cognitive bias --- as writers may expect any casual reader to easily verify, and thus criticize, the fidelity to the original quotation.
Quotation evolution is therefore a perfect environment to measure cognition-induced transformations and relate those findings to macroscopic social dynamics.

\subsection{Dataset}

We used a reliable quotation dataset collected by \citet{Leskovec09}, large enough to lend itself to statistical analysis.
This dataset consists of the daily crawling of news stories and blog posts from around a million online sources, with an approximate publication rate of 900k texts per day, over a nine-month period of time (from August 2008 to April 2009) \cite{Leskovec09-url}.\footnote{Unfortunately, the original article~\citep{Leskovec09} does not provide additional details on the source selection methodology.}
Quotations were then automatically extracted from this corpus: each quotation is a more or less faithful excerpt of an utterance (oral or written) by the quoted person. \cam{btw do we have an excerpt of quotations here, could we even feature a few prototypical examples?}
Quotations were then gathered in a graph and connected according to their similarity: either because they differ by very few words (in that case, no more than one word) or because they share a certain sequence of words (in that case, at least ten consecutive words).
A community detection algorithm was applied to that quotation graph to detect aggregates of tightly connected, i.e. sufficiently similar, groups of quotations (see \citet{Leskovec09} for more detail).
This analysis yielded the final data we had access to, with a total of about \num{70000} sets of quotations; each of these sets allegedly contains all variations of a same parent utterance, along with their respective publication URLs and timestamps.

\subsection{Word-level measures}

To keep the analysis palatable, we restricted the analysis to quotation transformations which consisted in the \emph{substitution} of a word by another word (and only those cases) in order to unambiguously discuss single word replacements. 
To quantify those substitutions, we decided to associate a number of features to each word, the variation of which we can statistically study.
The following sections detail the features we used.

\subsubsection{Standard psycholinguistic indices}

We first introduce some of the most classical psycholinguistic measures on words:

\rk{Add some bibliography about those features' known effects}

\begin{itemize}
    \item \textbf{Word frequency}: the frequency at which words appear in our dataset, \rk{add ref on why it's important}
    \item \textbf{Age of Acquisition}: the average age at which words are learned, obtained from~\citet{kuperman12},
    \item The average \textbf{Number of Phonemes} for all pronunciations of a word, obtained from the Carnegie Mellon University Pronouncing Dictionary~\citep{Weide98},\footnote{The CMU Pronouncing Dictionary is included in the NTLK package~\citep{Bird09}, the natural language processing toolkit we used for the analysis.}
    \item The average \textbf{Number of Syllables} for all pronunciations of a word, also obtained from the CMU Pronouncing Dictionary,
    \item The average \textbf{Number of Synonyms} for all meanings of a word, obtained from WordNet~\citep{WordNet10}. \rk{add ref on why it's important}
\end{itemize}

We also considered grammatical types within quotations by detection of \emph{Part-of-Speech} (POS) categories, using the Penn TreeBank Project typology~\citep{Santorini90} and thereby distinguishing verbs, nouns, adjectives and adverbs.
The results were however extremely similar across the various categories, exhibiting no specific effect of words belonging to different POS categories.
\rk{See \#8 for this fact-check}

\subsubsection{Network-based measures}

Aside from classical psycholinguistic measures, we also considered more recently studied variables based on semantic network properties.
We relied on the ``free association'' norms collected by~\citet{Nelson04} which naturally embed information on the idea association process underlying transformation of quotations.

Free association (FA) norms record the words that come to mind when someone is presented with a given cue (that is the ``free association'' task).
As \citeauthor{Nelson04} explain,
\begin{quote}
free association response probabilities index the likelihood that one word can cue another word to come to mind with minimal contextual constraints in effect.~\citep{Nelson04}
\end{quote}
%Following \citet{Griffiths07}, we first consider the directed weighted network formed by the association norms, that is the network where words are nodes and edges are directed from cue to associated word, with a weight equal to the probability of that target word being produced when this particular cue was presented.
% \rk{we're in fact using the unweighed version of the network. Why?}
\new{Following \citet{Griffiths07}, we first build a directed unweighted network based on association norms, where words are nodes and edges are directed from cue to target word whenever a target word is being produced when this particular cue word was presented.}
This network is of particular interest since it measures the \emph{in-vitro forced-choice} version of a substitution whereas the data we analyse is the \emph{in-vivo spontaneous} version of what we otherwise hypothesize to be the same process.

\bigskip
We introduce three standard network-based measures to be used on the FA network% \rk{adapt once the weighing question (\#8) is settled}
:

\begin{itemize}
    \item \textbf{Centrality} $k$, initially measured by the number of incoming edges to a given node, i.e. the number of cues for which a given word is triggered as an association, which strongly relates to word polysemy.
    However in the present case there is a quasi-perfect correlation between node incoming degree and node \emph{pagerank}~\citep{Page99}, which will lead us to favour the latter later on. Word pagerank on the FA network had already been used by~\citet{Griffiths07}; it may be interpreted as a generalized and recursive measure of word polysemy: central nodes in the pagerank sense are words often selected as targets when presented with cues themselves often selected as targets, and so on recursively.
    \item \textbf{Clustering coefficient} $c$, which measures the extent to which a node belongs to a local aggregate of tightly connected nodes, and defined as the ratio between the number of actual \emph{v.} possible edges between a node's neighbours \cite{watt-coll}.
    We compute the clustering coefficient on the undirected version of the FA network; we thus measure if a word belongs more or less to a local aggregate of equivalent words (from a ``free association'' point of view).
    \item \textbf{Betweenness coefficient} $b$, another measure of node centrality describing the extent to which a node tends to connect otherwise remote areas of the network~\citep{free:set}.
    More technically, it corresponds to the normalized number of shortest paths connecting dyads which pass through that node; the higher the coefficient, the more important that node is in ensuring the connectedness of the rest of the network.
    This quantity tells us if some words behave like unavoidable waypoints on the path associating one word to another.
\end{itemize}

\subsubsection{Variable correlations}

An important question arises concerning the possible correlations between all the variables we use.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\linewidth]{images/computed-figures/feature_correlations-filter1.png}
    \caption{Spearman correlations in the initial set of features}
    \label{fig:feature-corrs-initial}
\end{figure}

\rk{Correlation talk needs to be redone for new set of features}

Age of acquisition is a key variable which appears as a usual suspect in psycholinguistic studies and is also usually correlated to many of the other variables.
This relates to an ongoing debate suggesting that age of acquisition encodes a variety of phenomena, difficult to disentangle from more specific phenomena which could be captured by more independent variables~\CN{}.
Here however, as can be seen in Figure~\ref{fig:feature-corrs-initial}, age of acquisition has a relatively low correlation to the other variables (absolute value not above $0.42$ if we exclude the centrality measures), leading us to keep the variable in the rest of the analysis.

Number of phonemes and number of syllables naturally exhibit a strong linear correlation ($0.83$).
The analysis showed a better prediction effect of number of phonemes over number of syllables, which is consistent with~\citet{nick-diss}, and we therefore chose to focus the presented results on the former only.

Frequency and number of meanings both have relatively low levels of correlation to the other variables; we therefore also keep them in the rest of the analysis.

\bigskip
Network properties, on the other hand, are strongly dependent on one another.
As mentioned earlier, word degree and word pagerank have a very strong correlation ($0.89$) and, degree being generally more correlated to other variables, we chose to remove this variable from the results presented.
Finally betweenness centrality also exhibits strong correlation levels to the other network properties ($0.62$, $0.64$, and $0.72$ in absolute value), leading us to drop this final feature due to its redundancy.

The final set of variables we consider, as well as their cross-correlations, can be seen in Figure~\ref{fig:feature-corrs-filtered}.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.6975\linewidth]{images/computed-figures/feature_correlations-filter2.png}
    \caption{Spearman correlations in the filtered set of features}
    \label{fig:feature-corrs-filtered}
\end{figure}


\subsection{Temporal binning}

The data we use presents an additional challenge: each set of quotations bears no explicit information either about the authoritative original quotation, or about the source quotation(s) each author inspired himself from when creating a new post and reproducing (possibly altering) those sources.
Quote-to-quote transformations, and much less substitutions, are therefore not explicitly encoded in the dataset.

We face an inference problem where, given all quotations and their occurrence timestamps, we should estimate which was the originating quotation for each instance of each quotation.
We therefore model the underlying quotation selection process by making a few additional assumptions which let us define quote-to-quote substitutions from the available data.
The main issue at hand is deciding whether a later occurrence is a strict copy of an earlier occurrence, or a substitution of an even earlier occurrence, or perhaps even a substitution or copy from quotes appearing outside the dataset, \hbox{i.e.} from a source external to the data collection perimeter.

Let us give an example: say the quotation ``These accusations are false and \textbf{absurd}'' ($q_a$) appears in a blog on January 19, and the slightly different quotation ``These accusations are false and \textbf{incoherent}'' ($q_b$) appears in other blogs on the 21st, 22nd and 23rd of January.
If $q_a$ was sufficiently prominent when $q_b$ first appeared, we can safely assume that the first author of $q_b$ on the 21st based himself on $q_a$ as is shown in Figure~\ref{fig:substitution-temporal-binning-a}.
But what about the second and third occurrences of $q_b$, on the 22nd and 23rd?
Should we consider them to be substitutions based on $q_a$ %(i.e. re-creations of $q_b$ by a new instance of the substitution process that brought from $q_a$ to $q_b$ in the first place) 
or accurate reproductions of the previous occurrences of $q_b$? (Options shown in Figure~\ref{fig:substitution-temporal-binning-a}.)

\begin{figure}[h]
    \centering
    \subfloat[Possible paths from occurrence to occurrence]{
	    \def\svgwidth{\linewidth}
	    \small
	    \input{images/substitution-temporal-binning-a.pdf_tex}
	    \label{fig:substitution-temporal-binning-a}
	}
	\hfill \\
    \subfloat[Binned quotation family]{
	    \def\svgwidth{\linewidth}
	    \small
	    \input{images/substitution-temporal-binning-b.pdf_tex}
	    \label{fig:substitution-temporal-binning-b}
	}
	\caption{Temporal binning of quotation families}
    \label{fig:substitution-temporal-binning}
\end{figure}

To settle this question we bin the quote occurrences into fixed \emph{time bags} spanning $\Delta t$ days (2 days in the implementation), each one representing a unit of time evolution.
Then when a quotation $q$ appears in time bag $n$, it is counted as a substitution from each quote $q^*$ in the preceding time bag ($n - 1$) from which it differs by only one word.
If no quote in the preceding time bag can qualify as a source in a substitution (i.e. $q$ differs from all the quotes in the preceding time bag by more than one word), the occurrence of $q$ is not considered to be an instance of substitution.
Such a model defines how many times quote occurrences can be counted as substitutions: in Figure~\ref{fig:substitution-temporal-binning-b}, occurrences of $q_b$ on the 21st and 22nd are counted as substitutions, whereas the occurrence on the 23rd is not.

The assumptions embedded in this model are only a subset of a wider set of possibilities, each leading to alternative substitution inferences.\footnote{In particular, time can be sliced into bins to build fixed time bags as is done here, or kept fine-grained by using sliding time bags.}
These various flavours of an ideal substitution detection model essentially change whether occurrences are considered as substitutions from another quote, repetitions of the original quote, or introduction of information external to the dataset.
We identified and implemented eleven other such models, and they all yielded essentially the same results.

\section{Macroscopic evolution of quotation families}

We first examine the evolution of quotation families under the repeated action of substitutions.
Our goal in this step is to identify the long-term effect of cognitive bias over the lifetimes of quotation families and thus over the framing of public information.

To do so we compute the distribution of word features for each time bag $\mathcal{B}_n$ of each quotation family, and sum those distributions over all quotation families.
This yields a distribution of feature values for each $n$, which is the simplest possible view of the state of an average quotation family in its $n$-th time bag, i.e. after $n \times \Delta t$ days.\footnote{For consistency, if we do this for $n$ going up to $N$, we only include quotation families that span long enough to have at least $N$ time bags.}
Such a computation, based solely on the binning of quotation families, makes no assumptions on the way quotations undergo substitutions over time.

The raw distributions built with this computation are stationary.
That is, the substitutions on quotations over time have no global effect on quotation families, either because external quotations are continuously fed into the family and compensate for the effect of substitutions, or because substitutions operate on a marginal portion of the quotation families, or both.

\bigskip
To narrow this view to the specific effects of cognitive bias we consider substitution \emph{chains}: using the substitution detection model described in the previous section, we filter time bags to include only new quotes produced by substitutions themselves based on quotes produced by substitutions, and so on recursively.
The first time bag is therefore untouched, the second contains the quotations produced by substitutions from the first, the third by substitutions from the second, and so on (see Figure~\ref{fig:substitution-chains} for an illustration of this process).
As a result, the number of observed words drastically decreases across time, yet it unambiguously focuses on successive mutations.

\begin{figure}[h]
    \centering
    \def\svgwidth{\linewidth}
    \tiny
    \input{images/substitution-chains.pdf_tex}
    \caption{XXXXX}
    \label{fig:substitution-chains}
\end{figure}

We thus observe only the repeatedly-affected portion of the quotation families, and obtain a simple view of the longitudinal effect of cognitive bias, \hbox{i.e.} how quote features are evolving in the long term and, perhaps, converging towards specific attractors.

\rk{Comment results and transition to micro process}

\begin{figure*}[!tbh]
    \centering
    \includegraphics[width=\textwidth]{images/computed-figures/timebags_evolution_recursive_min20_no-exclusion.png}
    \caption{\textbf{Feature distribution evolution:} evolution of the distribution of feature values in substitution chains over successive 2-day time bags (bags 0 to 18, i.e. days 0 to 37).
    The legends indicate the number of words left in each time bag; these decrease exponentially since only a fraction of the quotes undergo substitution at each step.
    After a period of time, each feature becomes concentrated in a specific range of its own.}
    \label{fig:timebags-evolution}
\end{figure*}

\section{Microscopic evolution: the substitution process}

We then focus on the individual substitution process at work when authors transform quotations, by examining the features of the substituted and substituting words in each substitution.
Note that since we only consider substitutions and not faithful copies, we measure the features of an alteration \emph{knowing that there has been an alteration}, and we do not take invariant quotations into account.
Indeed, in the first case we know there has been a human reformulation, whereas in the second case it is impossible to know whether there has been perfect human reformulation or simply digital copy-pasting of a source (``{\sc Ctrl-C}/{\sc Ctrl-V}'').

\TB{This model has two main effects.
First, the binning influences how many times quote occurrences can be counted as substitutions: in Figure~\ref{fig:substitution-temporal-binning-b}, occurrences of $q_b$ on the 21st and 22nd are counted as substitutions, whereas the occurrence on the 23rd is not.
Second, the ``majority'' rule defines how quotes are sourced in substitutions: in the third drawing on Figure~\ref{fig:substitution-temporal-binning}, $q_b$ holds the majority and is the considered the basis for the last occurrence of $q_a$, in spite of $q_a$ having appeared earlier at the very beginning (indeed in the situation shown in Figure~\ref{fig:substitution-temporal-binning}, this seems to be the most likely scenario).}

\TB{\textbf{Source quotes} in the source interval for substitutions can be further restricted, e.g. to only the most frequent quote in the interval as in the model described above,}

\begin{figure}[h]
    \centering
    \def\svgwidth{\linewidth}
    \small
    \input{images/substitution-q_max.pdf_tex}
    \caption{XXXXX}
    \label{fig:substitution-q_max}
\end{figure}

We build two main observables for each word feature.
First, we measure the susceptibility for words to be the source of a substitution, knowing that there has been a variation, in order to show which semantic features are the most likely to attract a substitution under this condition.
Second, we measure the variation of word feature over a substitution, looking at the variation of a given feature between start and arrival words.

\subsection{Susceptibility}

For a given feature $\phi$, the protocol lets us compute substitution \emph{susceptibilities} for each feature value $f$.
We say that a word is \emph{substitutable} if it appears in a quote which undergoes a substitution, whether that substitution operates on the considered word or on another.
Word substitution susceptibility is computed as the ratio of the number $s_w$ of times a word is substituted to the number $p_w$ of times that word appears in a substitutable position, i.e. $\sfrac{s_w}{p_w}$.

Now averaging over all words such that $\phi(w) = f$ (only taking into account words that are substituted at least once), we obtain the mean susceptibility for the feature value $f$:
\footnote{To avoid any auto-correlation effect due to the number of substitutions in a cluster (possibly leading to an overly optimistic estimation of confidence intervals), we first average substitutions over each cluster, by considering the average of arrival word features for a given start word.
Indeed, substitutions occurring in the same cluster are likely not statistically independent.}
$$\sigma_{\phi}(f) = \left< \frac{s_w}{p_w} \right>_{\left\lbrace w | \phi(w) = f \right\rbrace}$$

This measure focuses on the selection of start words involved in substitutions, measuring the effect of features at the moment preceding the substitution when it is not yet known which word in the quotation -- if any -- will be substituted.

\subsection{Alteration}

Next, we measure how a word $\wstart$'s feature varies as $\wstart$ is substituted by $\warrival$, i.e. $\phi(\warrival) - \phi(\wstart)$.
Averaging this value over all start words such that $\phi(w) = f$ yields the mean variation for that feature value~$f$:
$$\Delta_{\phi}(f) = \left< \phi(\warrival) - \phi(\wstart) \right>_{\left\lbrace (\wstart,\warrival) | \phi(\wstart) = f \right\rbrace}$$

We introduce a null hypothesis $\mathcal{H}_0$ to compare the actual variation of a word's feature to its expected variation, assuming the arrival word $\warrival_0$ had been chosen randomly from the pool of free association words.
The new quantity under $\mathcal{H}_0$ is:
\footnote{Note that $\phi(\warrival_0)$ is in fact a constant in this averaging, since by definition $\warrival_0$ does not depend on $\wstart$.}
$$\Delta_{\phi}^0 (f) = \left< \phi({\warrival_0}) - \phi(\wstart) \right>_{\left\lbrace (\wstart,\warrival_0) | \phi(\wstart) = f \right\rbrace}$$

We also considered an alternative null hypothesis, denoted $\mathcal{H}_{00}$, where the arrival word is chosen randomly \emph{among immediate synonyms of the start word}, i.e. an arrival word chosen among semantically plausible though still random words.
\footnote{In this case $\warrival_{00}$ does depend on $\wstart$.} \rk{Do we present these results?}\cam{nope}\rk{S: Then we must say the results are the same.}

Using this method we obtain the mean variation of feature for each start feature value, and can compare the variations to a situation where arrival words are chosen randomly.
This gives us a fine-grained view of how word features evolve upon substitution.

\subsection{Results}

\rk{Comment results}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{images/computed-figures/feature_susceptibilities.png}
    \caption{\textbf{Substitution susceptibility:} average susceptibility to substitution \emph{v.} average feature value of a candidate word for substitution, with 95\% asymptotic confidence intervals.
    Each feature exhibits a specific and significant pattern favouring either high- or low-valued words for substitution.}
    \label{fig:feature-susceptibilities}
\end{figure*}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{images/computed-figures/feature_variations-binned.png}
    \caption{\textbf{Feature variation upon substitution:} average feature of the appearing word minus $\mathcal{H}_0$ \emph{v.} average feature of the disappearing word in a substitution, with 95\% asymptotic confidence intervals.
    The overall position of the curve with respect to $y = 0$ indicates the direction of the cognitive bias.
    The fact that all the curves have slopes smaller than 1 means that the substitution operation is contractile on average: each feature will converge towards its own specific asymptotic range, which is consistent with the evolution observed in Figure~\ref{fig:timebags-evolution}.}
    \label{fig:feature-variations}
\end{figure*}
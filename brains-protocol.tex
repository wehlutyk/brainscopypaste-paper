% !TEX root = brainscopypaste.tex
% ============================
\section{Protocol} % =========
% ============================
\label{sec:protocol}

In order to start bridging this gap, we set out to \emph{empirically} study public representation transformations at the microscopic level, aiming to stay compatible with macroscopic-level studies of these public representations.
Quotations appeared to be a perfect candidate as public representations.
First, they are usually cleanly delimited by quotation marks (and often with HTML markup in web pages), which greatly facilitates their detection in text corpora.
Second, they stem from a unique ``original'' version, and could ideally be traceable back to that version.
Third, and most importantly, their duplication should \emph{a priori} be highly faithful, apart from cases of cropping: not only should transformations be of moderate magnitude, but when specific words are not perfectly duplicated, it is safe to assume that the variation is due to involuntary cognitive bias --- as writers may expect any casual reader to easily verify, and thus criticize, the fidelity to the original quotation.
Quotation evolution is therefore a perfect environment to measure cognition-induced transformations and relate those findings to macroscopic social dynamics.

\subsection{Dataset}

We relied on a reliable quotation dataset collected by \citet{Leskovec09}, large enough to lend itself to statistical analysis.
This dataset consists of the daily crawling of news stories and blog posts from around a million online sources, with an approximate publication rate of 900k texts per day, over a nine-month period of time (from August 2008 to April 2009) \cite{Leskovec09-url}.\footnote{Unfortunately, the original article~\citep{Leskovec09} does not provide additional details on the source selection methodology.}
Quotations were then automatically extracted from this corpus: each quotation is a more or less faithful excerpt of an utterance (oral or written) by the quoted person.
Quotations were then gathered in a graph and connected according to their similarity: either because they differ by very few words (in that case, no more than one word) or because they share a certain sequence of words (in that case, at least ten consecutive words).
A community detection algorithm was applied to that quotation graph to detect aggregates of tightly connected, i.e. sufficiently similar, groups of quotations (see \citet{Leskovec09} for more detail).
This analysis yielded the final data we had access to, with a total of about \num{70000} sets of quotations; each of these sets allegedly contains all variations of a same parent utterance, along with their respective publication URLs and timestamps.

\subsection{Word-level measures}

To keep the analysis palatable, we restricted the analysis to quotation transformations which consisted in the \emph{substitution} of a word by another word (and only those cases).
To quantify those substitutions, we decided to associate a number of features to each word, the variation of which we can statistically study.
The following sections detail the features we used.

\subsubsection{Standard psycholinguistic indices}

We first introduce some of the most classical psycholinguistic measures on words:

\rk{Add some bibliography about those features' known effects}

\begin{itemize}
    \item \textbf{Word frequency}: the frequency at which words appear in our dataset, \rk{add ref saying it's important}
    \item \textbf{Age of Acquisition}: the average age at which words are learned, obtained from~\citet{kuperman12},
    \item The average \textbf{Number of Phonemes} for all pronunciations of a word, obtained from the Carnegie Mellon University Pronouncing Dictionary~\citep{Weide98},\footnote{The CMU Pronouncing Dictionary is included in the NTLK package~\citep{Bird09}, the natural language processing toolkit we used for the analysis.}
    \item The average \textbf{Number of Syllables} for all pronunciations of a word, also obtained from the CMU Pronouncing Dictionary,
    \item The \textbf{Number of Meanings} for a word, obtained from WordNet~\citep{WordNet10}. \rk{add ref on why it's important}
\end{itemize}

We also considered grammatical types within quotations by detection of \emph{Part-of-Speech} (POS) categories, using the Penn TreeBank Project typology~\citep{Santorini90} and thereby distinguishing verbs, nouns, adjectives and adverbs.
The results were however extremely similar across the various categories, exhibiting no specific effect of words belonging to different POS categories.
\rk{See \#8 for this fact-check}

\subsubsection{Network-based measures}

Additional to classical psycholinguistic measures, we also considered more recently studied variables based on semantic network properties.
We relied on the ``free association'' norms collected by~\citet{Nelson04} which naturally embed information on the idea association process underlying transformation of quotations.

Free Association (FA) norms record the words that come to mind when people are presented with a given cue (that is the ``free association'' task).
As \citeauthor{Nelson04} explain,
\begin{quote}
free association response probabilities index the likelihood that one word can cue another word to come to mind with minimal contextual constraints in effect.~\citep{Nelson04}
\end{quote}
Following \citet{Griffiths07}, we first consider the directed weighed network formed by the association norms, that is the network where words are nodes and edges are directed from cue to associated word, with a weight equal to the probability of that target word being produced when this particular cue was presented.
\rk{we're in fact using the unweighed version of the network. Why?}
This network is of particular interest since it measures the \emph{in-vitro forced-choice} version of a substitution whereas the data we analyse is the \emph{in-vivo spontaneous} version of what we otherwise hypothesize to be the same process.

\bigskip
We introduce three standard network-based measures to be used on both the FA network \rk{adapt once the weighing question (\#8) is settled}:

\begin{itemize}
    \item \textbf{Centrality} $k$, initially measured by the number of incoming edges to a given node, i.e. the number of cues for which a given word is triggered as an association, which strongly relates to word polysemy.
    However in the present case there is a quasi-perfect correlation between node incoming degree and node \emph{pagerank}~\citep{Page99}, which will lead us to favour the latter later on. Word pagerank on the FA network had already been used by~\citet{Griffiths07}; it may be interpreted as a generalized and recursive measure of word polysemy: central nodes in the pagerank sense are words often selected as targets when presented with cues themselves often selected as targets, and so on recursively.
    \item \textbf{Clustering coefficient} $c$, which measures the extent to which a node belongs to a local aggregate of tightly connected nodes, and defined as the ratio between the number of actual v. possible edges between a node's neighbours \cite{watt-coll}.
    We compute the clustering coefficient on the undirected version of the FA network; we thus measure if a word belongs more or less to a local aggregate of equivalent words (from a ``free association'' point of view).
    \item \textbf{Betweenness coefficient} $b$, another measure of node centrality describing the extent to which a node tends to connect otherwise remote areas of the network~\citep{free:set}.
    More technically, it corresponds to the normalized number of shortest paths connecting dyads which pass through that node; the higher the coefficient, the more important that node is in ensuring the connectedness of the rest of the network.
    This quantity tells us if some words behave like unavoidable waypoints on the path associating one word to another.
\end{itemize}

\subsubsection{Variable correlations}

There is an important questions here about the possible correlations between all the variables we consider.

\begin{figure}[!th]
    \centering
    \includegraphics[width=\linewidth]{images/computed-figures/feature_correlations-filter1.png}
    \caption{Correlations in the initial set of features \rk{show non log version?}}
    \label{fig:feature-corrs-initial}
\end{figure}

Age of acquisition is a key variable which appears as a usual suspect in psycholinguistic studies and is also usually correlated to many of the other variables.
This relates to an ongoing debate suggesting that age of acquisition encodes a variety of phenomena, difficult to disentangle from more specific phenomena which could be captured by more independent variables such as~\CN{}.
Here however, as can be seen in Figure~\ref{fig:feature-corrs-initial}, age of acquisition has a relatively low correlation to the other variables (absolute value not above $0.42$ if we exclude the centrality measures), leading us to keep the variable in the rest of the analysis.

Number of phonemes and number of syllables naturally exhibit a strong linear correlation ($0.83$).
The analysis showed a better prediction effect of number of phonemes over number of syllables, which is consistent with~\citet{nick-diss}, and we therefore chose to focus the presented results on the former only.

Frequency and number of meanings both have relatively low levels of correlation to the other variables; we therefore also keep them in the rest of the analysis.

\bigskip
Network properties, on the other hand, are strongly dependant on one another.
As mentioned earlier, word degree and word pagerank have a very strong correlation ($0.89$) and, degree being generally more correlated to other variables, we chose to remove this variable from the results presented.
Finally betweenness centrality also exhibits strong correlation levels to the other network properties ($0.62$, $0.64$, and $0.72$ in absolute value), leading us to drop this final feature due to its redundancy.

The final set of variables we consider, as well as their cross-correlations, can be seen in Figure~\ref{fig:feature-corrs-filtered}.

\begin{figure}[!th]
    \centering
    \includegraphics[width=0.788\linewidth]{images/computed-figures/feature_correlations-filter2.png}
    \caption{Correlations in the filtered set of features \rk{show non log version?}}
    \label{fig:feature-corrs-filtered}
\end{figure}


\subsection{Detection of transformations}

The data we use presents an additional challenge: each set of quotations bears no explicit information either about the authoritative original quotation, or about the source quotation(s) each author inspired himself from when creating a new post and reproducing (possibly altering) those sources.
Quote-to-quote transformations, and much less substitutions, are therefore not explicitly encoded in the dataset.

We face an inference problem where, given all quotations and their occurrence timestamps, we must estimate which was the originating quotation for each instance of each quotation.
We therefore model the underlying quotation selection process by making a few additional assumptions which let us define quote-to-quote transformations and substitutions from the available data.

\rk{Rework all this with model classification}

\TB{The main question in this problem concerns the way we consider later occurrences of a quotation which, when it first appeared, was identified to be an alteration from an original quotation.
Let us give an example: say the quotation ``These accusations are false and absurd'' ($q_a$) appears in a blog on January 18, and the slightly different quotation ``These accusations are false and incoherent'' ($q_b$) appears in another blog on the 19th, the 20th, and the 21st of January.
If $q_a$ was sufficiently prominent when $q_b$ first appeared, we can safely assume that the author of $q_b$ on the 19th based himself on $q_a$.
But what about the following occurrences of $q_b$?
Should we consider them to be substitutions based on $q_a$ (i.e. re-creations of $q_b$ by a new instance of the substitution process that brought from $q_a$ to $q_b$ in the first place) or reproductions of the first occurrence of $q_b$?}

\TB{To settle this question we model the process as follows: we assume that when a quotation $q$ appears at time $t$, if it is not original (i.e. if not stemming from a source external to the dataset, e.g. initiating a new set of quotations), then it is based solely on the most frequent quotation $q_{max}$ in the preceding period of time $[t - \Delta t ; t]$.
The length $\Delta t$ of that period of time is fixed as a fraction of the total duration of the set of quotations; we took one fifth in the implementation (i.e. one month for a set of quotations spanning five months, or one day for a set spanning only five days; this takes the dynamism of the set of quotations into account).
If $q$ differs from $q_{max}$ by only a word, it is counted as a substitution from $q_{max}$ to $q$.
In any other case, i.e. if $q$ and $q_{max}$ are the same or if $q$ and $q_{max}$ are different in other ways, the occurrence of $q$ is not considered to be an instance of substitution and is discarded.}

\TB{An example of this detection in a situation akin to the one described above can be seen on figure \ref{fig:model-slidetimebags}: $q_a$ and $q_b$ differ by only a word, and $q_a$ appears first and stays the most frequent in the beginning.
This is why the occurrences of $q_b$ at $t_1$ and $t_2$ are detected as substitutions stemming from $q_a$.
After a time the situation is reversed: $q_b$ becomes more frequent than $q_a$.
This entails that occurrences of $q_b$ are seen as reproductions of itself ($t_3$), and that occurrences of $q_a$ are detected as substitutions stemming from $q_b$ ($t_4$), i.e. \emph{re-creations} of $q_a$.}

\rk{Add figure describing the chosen model}

\TB{The assumptions embedded in this model are only a subset of a wider set of possibilities, each leading to alternative models.
We identified and implemented five other such models, and they all yielded essentially the same results.
These models differ in the definition of the source quote from which new occurrences stem, essentially modifying the balance between reproduction of previous occurrences of a quote and re-creation of itself by a new substitution instance.
For example, the time-windows considered can have different lengths, can include all occurrences from the beginning of the set of quotations, or can have fixed positions.
The source quotation of a potential substitution can be chosen among those time-windows, or otherwise (e.g. among \emph{all} quotations having appeared before $t$ and differing by a word from the arrival quotation; this detection process would include all possible substitutions detected by other models, but would also include many false positives).}

\subsection{Global evolution of quotation families}



\subsection{Transformation process}

We then focus on the transformation process at work when authors reproduce quotations, by examining the features of substituted and substituting words in each substitution.
Note that since we only consider substitutions and not faithful copies, we measure the features of an alteration \emph{knowing that there has been an alteration}, and we do not take invariant quotations into account.
Indeed, in the first case we know there has been a human reformulation, whereas in the second case it is impossible to know whether there has been perfect human reformulation or simply digital copy-pasting of a source (``{\sc Ctrl-C}/{\sc Ctrl-V}'').

We build two main observables for each word feature.
First, we measure the susceptibility for words to be the source of a substitution, knowing that there has been a variation, in order to show which semantic features are the most likely to attract a substitution.
Second, we measure the variation of word feature over a substitution, looking at the variation of a given feature between start and arrival words.

\subsubsection{Susceptibility}

For a given feature $\mathfrak{F}$, the protocol lets us compute substitution \emph{susceptibilities} for each feature value $f$.
We say that a word is \emph{substitutable} if it appears in a quote which undergoes a substitution, whether that substitution operates on the considered word or on another.
Word substitution susceptibility (denoted $\mathfrak{S}_{\mathfrak{F}}(w)$) is computed as the ratio of the number of times $n_s(w)$ a word is substituted to the number of times $n_p(w)$ that word appears in a substitutable position.
We have:
$$\mathfrak{S}_{\mathfrak{F}}(w) \defeq \frac{n_s(w)}{n_p(w)}$$

Now averaging over all words having a given feature value $f$, we obtain the mean susceptibility for the feature value $f$:
\footnote{To avoid any auto-correlation effect due to the number of substitutions in a cluster (possibly leading to an overly optimistic estimation of confidence intervals), we first average substitutions over each cluster, by considering the average of arrival word features for a given start word.
Indeed, substitutions occurring in the same cluster are likely not statistically independent.}
$$\left< \mathfrak{S}_{\mathfrak{F}} \right>_f = \left< \frac{n_s(w)}{n_p(w)} \right>_{\left\lbrace w | \mathfrak{F}(w) = f \right\rbrace}$$

This measure focuses on the selection of start words involved in substitutions, measuring the effect of features at the moment preceding the substitution when it is not yet known which word in the quotation -- if any -- will be substituted.

\subsubsection{Alteration}

Next, we measure how a word $\wstart$'s feature varies as $\wstart$ is substituted by $\warrival$.
Let us denote this quantity:
$$\Delta (\wstart,\warrival) \defeq \mathfrak{F}(\warrival) - \mathfrak{F}(\wstart)$$

Averaging this value over all start words with a given feature value $f$ yields the mean variation for that feature value $f$.
This quantity can be written:
$$\left< \Delta(\wstart,\warrival) \right>_f = \left< \mathfrak{F}(\warrival) - \mathfrak{F}(\wstart) \right>_{\left\lbrace (\wstart,\warrival) | \mathfrak{F}(\wstart) = f \right\rbrace}$$

We introduce a null hypothesis $\mathcal{H}_0$ to compare the actual variation of a word's feature to its expected variation, assuming the arrival word $\warrival_{\mathcal{H}_0}$ had been chosen randomly from the pool of free association words.
The corresponding average quantity over all start words may be written:
\footnote{Note that $\warrival_{\mathcal{H}_0}$ is in fact constant in this averaging, since by definition it does not depend on $\wstart$.}
$$\left< \Delta(\wstart,\warrival_{\mathcal{H}_0}) \right>_f = \left< \mathfrak{F}({\warrival_{\mathcal{H}_0}}) - \mathfrak{F}(\wstart) \right>_{\left\lbrace (\wstart,\warrival_{\mathcal{H}_0}) | \mathfrak{F}(\wstart) = f \right\rbrace}$$

We also considered an alternative null hypothesis, denoted $\mathcal{H}_{00}$, where the arrival word is chosen randomly \emph{among immediate synonyms of the start word}, i.e. an arrival word chosen among semantically plausible though still random words.
\footnote{In this case $\warrival_{\mathcal{H}_{00}}$ does depend on $\wstart$.} \rk{Do we present these results?}

Using this method we obtain the mean variation of feature for each start feature value, and can compare the variations to a situation where arrival words are chosen randomly.
This gives us a fine-grained view of how word features evolve upon substitution.